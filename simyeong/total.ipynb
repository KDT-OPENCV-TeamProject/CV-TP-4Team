{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../seungmin/DATA/Dataset/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 데이터셋 로드\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# train 데이터셋\u001b[39;00m\n\u001b[1;32m     16\u001b[0m train_img_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../seungmin/DATA/Dataset/train/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_img_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# test 데이터셋\u001b[39;00m\n\u001b[1;32m     20\u001b[0m test_img_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../seungmin/DATA/Dataset/test/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch_PY38/lib/python3.8/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../seungmin/DATA/Dataset/train/'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# 이미지 데이터셋, 전처리 모듈\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),          # 이미지를 흑백으로 변환\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((50, 50))      # 사이즈 50, 50으로 고정\n",
    "])\n",
    "# 데이터셋 로드\n",
    "# train 데이터셋\n",
    "train_img_root = '../seungmin/DATA/Dataset/train/'\n",
    "train_dataset = ImageFolder(root=train_img_root, transform=transform)\n",
    "\n",
    "# test 데이터셋\n",
    "test_img_root = '../seungmin/DATA/Dataset/test/'\n",
    "test_dataset = ImageFolder(root=test_img_root, transform=transform)\n",
    "train_dataset\n",
    "# class 즉 타겟 확인\n",
    "train_dataset.classes\n",
    "# 한 번만 돌려서 형태 확인\n",
    "for feature, target in train_dataset:\n",
    "    print(feature.shape, target)\n",
    "    break\n",
    "# CNN 모델 만들기\n",
    "class VehicleCNN(nn.Module):\n",
    "      \n",
    "    def __init__(self, num_classes=3):  # num_classes는 분류하고자 하는 클래스가 육, 해, 공 3종류이므로 3으로 설정\n",
    "        super(VehicleCNN, self).__init__()\n",
    "        # 첫 번째 합성곱 레이어: 입력 채널 1, 출력 채널 64, 커널 크기 3, 스트라이드 1, 패딩 'same'\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding='same')   # 입력 채널 1 (grayscale 이미지) -> 최후수단 padding='same'\n",
    "        # 최대 풀링 레이어: 커널 크기 2, 스트라이드 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 첫 번째 드롭아웃 레이어: 드롭아웃 비율 0.25\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        # 두 번째 합성곱 레이어: 입력 채널 64, 출력 채널 128, 커널 크기 3, 스트라이드 1, 패딩 'same'\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same') # 위에서 out_channels 64-> 32-> 16 -> 8 ->10 -> 64 ->128로 바꿈 \n",
    "        # 두 번째 드롭아웃 레이어: 드롭아웃 비율 0.25\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # 세 번째 합성곱 레이어: 입력 채널 64, 출력 채널 128, 커널 크기 3, 스트라이드 1, 패딩 'same'\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same') # 위에서 out_channels 64-> 32-> 16 -> 8 ->10 -> 64 ->128로 바꿈 \n",
    "        # 세 번째 드롭아웃 레이어: 드롭아웃 비율 0.25\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        \n",
    "        # 첫 번째 완전 연결 레이어: 입력 특성 128*12*12, 출력 특성 50\n",
    "        self.fc1 = nn.Linear(128 * 6 * 6, 50)                       # 120 -> 60 -> 30 -> 50\n",
    "        # 세 번째 드롭아웃 레이어: 드롭아웃 비율 0.5\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        \n",
    "        # 두 번째 완전 연결 레이어: 입력 특성 50, 출력 특성 num_classes\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 첫 번째 합성곱 적용\n",
    "        x = self.conv1(x)\n",
    "        # 활성화 함수 ReLU 적용\n",
    "        x = F.relu(x)\n",
    "        # 최대 풀링 적용\n",
    "        x = self.pool(x)\n",
    "        # 첫 번째 드롭아웃 적용\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # 두 번째 합성곱 적용\n",
    "        x = self.conv2(x)\n",
    "        # 활성화 함수 ReLU 적용\n",
    "        x = F.relu(x)\n",
    "        # 최대 풀링 적용\n",
    "        x = self.pool(x)\n",
    "        # 두 번째 드롭아웃 적용\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # 세 번째 합성곱 적용\n",
    "        x = self.conv3(x)\n",
    "        # 활성화 함수 ReLU 적용\n",
    "        x = F.relu(x)\n",
    "        # 최대 풀링 적용\n",
    "        x = self.pool(x)\n",
    "        # 세 번째 드롭아웃 적용\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # 특성 벡터로 변환 (평탄화)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # 첫 번째 완전 연결 레이어 적용\n",
    "        x = self.fc1(x)\n",
    "        # 활성화 함수 ReLU 적용\n",
    "        x = F.relu(x)\n",
    "        # 세 번째 드롭아웃 적용\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # 두 번째 완전 연결 레이어 적용\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "# 학습 준비\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = VehicleCNN(num_classes=len(train_dataset.classes))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # --> vald 값이 조절이 일정수치 이하로 안 떨어지면 lr값 변화 시도 (0.001 -> 0.1)\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 100           # 50 -> 30-> 100 (다시 늘린 이유 : 30에서 에포크 횟수가 늘어날수록 loss값이 아름답게 줄어들어서)\n",
    "stopCall = 10          # 5 -> 10으로 변경\n",
    "# 스케쥴러 설정\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # 30 EPOCHS마다 학습률을 0.1배 감소\n",
    "torch.random.manual_seed(12)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [0.7,0.3])\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=30, shuffle=True)   # shuffle=True 는 무작위 추출\n",
    "val_loader = DataLoader(val_dataset, batch_size=30, shuffle=True)      # 마찬가지(시계열 데이터가 아니므로 무작위 설정) / 과대적합 방지로 val_loader\n",
    "# 모델 학습\n",
    "best_val_loss = float('inf')  # 최적의 검증 손실값을 저장할 변수 초기화\n",
    "earlystop = 0                 # 조기 종료 카운터 초기화\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()             # 모델을 학습 모드로 설정\n",
    "    loss_sum = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()           # 옵티마이저의 그라디언트 초기화\n",
    "        outputs = model(images)         # 모델에 입력을 주고 출력을 얻음\n",
    "        loss = cost(outputs, labels)    # 출력과 타겟 라벨 사이의 손실 계산\n",
    "        loss.backward()                 # 손실에 대한 모델의 그라디언트 계산\n",
    "        optimizer.step()                # 옵티마이저를 사용하여 파라미터 업데이트\n",
    "        \n",
    "        loss_sum += loss.item() * images.size(0)\n",
    "    \n",
    "    loss_avg = loss_sum / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss_avg:.4f}')\n",
    "    \n",
    "    # 검증 단계\n",
    "    model.eval()                        # 모델을 평가 모드로 설정\n",
    "    val_loss_sum = 0.0\n",
    "    \n",
    "    with torch.no_grad():               # 그라디언트 계산을 하지 않음\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            loss = cost(outputs, labels)\n",
    "            val_loss_sum += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss_avg = val_loss_sum / len(val_loader.dataset)\n",
    "    print(f'Validation Loss: {val_loss_avg:.4f}')\n",
    "    \n",
    "    # 스케줄러 단계 진행\n",
    "    scheduler.step(val_loss_avg)\n",
    "    \n",
    "    # 모델 체크포인트와 조기 종료 검사, 제일 좋은 모델 저장\n",
    "    if val_loss_avg < best_val_loss:\n",
    "        best_val_loss = val_loss_avg\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        earlystop = 0\n",
    "    else:\n",
    "        earlystop += 1\n",
    "\n",
    "    if earlystop > stopCall:\n",
    "        print(\"조기종료\")\n",
    "        break\n",
    "\n",
    "model.eval()      # 모델을 평가 모드로 설정\n",
    "final_loss = 0    # 예측값과 라벨 사이의 손실 계산 \n",
    "val_correct = 0       # 정답\n",
    "\n",
    "with torch.no_grad():  # 평가시 모델 업데이트 불필요 / 그라디언트 계산을 하지 않음\n",
    "    for images, labels in val_loader:\n",
    "        #print(images.shape)\n",
    "        outputs = model(images)\n",
    "        final_loss += cost(outputs, labels).item()  # 손실을 누적\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)  # 가장 높은 값을 가진 인덱스를 예측값으로 선택\n",
    "        val_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "final_loss /= len(val_loader.dataset)\n",
    "print(f'평균 손실: {final_loss:.4f}, Accuracy: {val_correct}/{len(val_loader.dataset)} ({100. * val_correct / len(val_loader.dataset):.0f}%)')\n",
    "# DataLoader 설정\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)   # shuffle=True 는 무작위 추출\n",
    "model.eval()      # 모델을 평가 모드로 설정\n",
    "test_loss = 0    # 예측값과 라벨 사이의 손실 계산 \n",
    "test_correct = 0       # 정답\n",
    "\n",
    "# with torch.no_grad():  # 평가시 모델 업데이트 불필요 / 그라디언트 계산을 하지 않음\n",
    "#     for images, labels in test_loader:\n",
    "#         #print(images.shape)\n",
    "#         outputs = model(images)\n",
    "#         test_loss += cost(outputs, labels).item()  # 손실을 누적\n",
    "#         pred = outputs.argmax(dim=1, keepdim=True)  # 가장 높은 값을 가진 인덱스를 예측값으로 선택\n",
    "#         test_correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "# test_loss /= len(test_loader.dataset)\n",
    "# print(f'평균 손실: {test_loss:.4f}, Accuracy: {test_correct}/{len(test_loader.dataset)} ({100. * test_correct / len(test_loader.dataset):.0f}%)')\n",
    "# 실제 예측\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 이미지 전처리를 위한 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),          # 이미지를 흑백으로 변환\n",
    "    transforms.Resize((50, 50)),     # 이미지 크기 조정\n",
    "    transforms.ToTensor()            # 이미지를 PyTorch 텐서로 변환\n",
    "])\n",
    "\n",
    "model_pth = '../seungmin/best_model.pth'  # best model의 주소\n",
    "# 학습된 모델을 로드하는 함수\n",
    "def load_trained_model(model_pth):\n",
    "    model = VehicleCNN(num_classes=3)  \n",
    "    model.load_state_dict(torch.load(model_pth))\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    return model\n",
    "img_pth = '../seungmin/DATA/pre/tank.jpg'\n",
    "# 이미지를 입력으로 받아 예측을 수행하는 함수\n",
    "def predict_image(img_pth, model, transform):\n",
    "    image = Image.open(img_pth)  # 이미지 파일을 열기\n",
    "    image = transform(image)        # 이미지 전처리\n",
    "    image = image.unsqueeze(0)      # 배치 차원 추가 ([1, C, H, W] 형태로 변환)\n",
    "    with torch.no_grad():  # 그라디언트 계산을 하지 않음\n",
    "        output = model(image)  # 모델을 통해 예측 수행\n",
    "        prediction = output.argmax(dim=1)  # 가장 높은 점수를 가진 클래스 선택\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "model = load_trained_model(model_pth)\n",
    "predicted_class = predict_image(img_pth, model, transform)\n",
    "\n",
    "what = ''\n",
    "if predicted_class == 0:\n",
    "    what = 'airplanes'\n",
    "elif predicted_class == 1:\n",
    "    what = 'cars'\n",
    "elif predicted_class == 2:\n",
    "    what = 'ship'\n",
    "print(f'예측 결과 : {what}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((300, 300))\n",
    "])\n",
    "# 데이터셋 로드\n",
    "img_root = 'data'\n",
    "dataset = ImageFolder(root=img_root, transform=transform)\n",
    "\n",
    "# 데이터셋 분할 (학습/검증/테스트)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 32\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# VGG 모델 로드 및 전이학습 설정\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "# VGG 첫 번째 컨볼루션 레이어의 입력 채널 수 수정\n",
    "vgg.features[0] = nn.Conv2d(1, 64, kernel_size=5, padding=1)\n",
    "\n",
    "for param in vgg.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "num_features = vgg.classifier[0].in_features\n",
    "vgg.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, len(dataset.classes))\n",
    ")\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# 모델 설정\n",
    "model = vgg.to(DEVICE)\n",
    "\n",
    "# 손실 함수 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 리스트 정의\n",
    "trainList = []\n",
    "valList = []\n",
    "testList = []\n",
    "# 학습 함수\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        trainList.append(loss)\n",
    "    return running_loss / len(loader)\n",
    "# 검증 함수\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            valList.append(loss)\n",
    "    return running_loss / len(loader)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "SCHEDULERS = ReduceLROnPlateau(optimizer, mode = 'min', patience = 3, verbose = True)\n",
    "cnt = 0\n",
    "\n",
    "# # 학습 루프\n",
    "# best_val_loss = 10000\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train(model, train_loader, optimizer, criterion)\n",
    "#     val_loss = validate(model, val_loader, criterion)\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "#     if SCHEDULERS.num_bad_epochs >= SCHEDULERS.patience:\n",
    "#         print(f\"Early stopping at epoch {epoch}\")\n",
    "#         break\n",
    "#     # print(epoch)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# 에포크 loss 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "x = range(1,len(trainList)+1)\n",
    "plt.plot(x, torch.tensor(trainList).detach().numpy(), label='Train Loss')\n",
    "x = range(1,len(valList)+1)\n",
    "plt.plot(x, torch.tensor(valList).detach().numpy(), label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 테스트 함수\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    testList.append(accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# 모델 로드 및 테스트\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_accuracy = test(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# 테스트 결과 출력\n",
    "\n",
    "img_path = 'test/image2.png'  # 테스트 이미지 경로\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 그레이스케일 변환\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((300, 300))\n",
    "])\n",
    "\n",
    "img = Image.open(img_path).convert('L')\n",
    "img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "plt.imshow(img, cmap='gray')  # 그레이스케일 이미지로 표시\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 결과 출력\n",
    "prob = torch.softmax(output, dim=1).detach().cpu().squeeze().numpy()\n",
    "max_prob = max(prob)\n",
    "rounded_prob = round(max_prob, 2)\n",
    "\n",
    "class_names = [\"ev6\",\"ev9\",\"qm3\",\"qm6\",\"sm3\",\"sm6\",\"test\",\"xm3\",\"그랜저\",\"넥쏘\",\"더 뉴 아반떼\",\"렉스턴\",\"스타리아\",\"싼타페\",\"쏘나타\",\"아우디 a4\",\"아우디 a5\",\"아우디 a6\",\"아우디 a7\",\"아우디 e-tron\",\"아우디 q3\",\"아우디 q7\",\"아우디 q8\",\"아이오닉5\",\"아이오닉6\",\"제네시스 g70\",\"제네시스 g90\",\"제네시스 gv70\",\"코나\",\"코란도\",\"토레스\",\"투싼\"]  # 클래스 이름 리스트 (실제 클래스 이름으로 변경해야 함)\n",
    "predicted_class = class_names[predicted.item()]\n",
    "\n",
    "print(f\"예측 클래스: {predicted_class}\")\n",
    "print(f\"예측 확률: {rounded_prob*100:.2f}%\")\n",
    "class_names = [\"test\",]  # 클래스 이름 리스트 (실제 클래스 이름으로 변경해야 함)\n",
    "brandList = {\n",
    "    \"현대\" : [\"그랜저\",\"더 뉴 아반떼\",\"스타리아\",\"아이오닉5\",\"아이오닉6\",\"넥쏘\",\"쏘나타\",\"투싼\", \"코나\",\"싼타페\",],\n",
    "    \"기아\" : [\"ev6\",\"ev9\",],\n",
    "    \"르노\" : [\"qm3\",\"qm6\",\"sm3\",\"sm6\",\"xm3\",],\n",
    "    \"제네시스\" : [\"제네시스 g70\",\"제네시스 g90\",\"제네시스 gv70\",],\n",
    "    \"아우디\"  : [\"아우디 a4\",\"아우디 a5\",\"아우디 a6\",\"아우디 a7\",\"아우디 e-tron\",\"아우디 q3\",\"아우디 q7\",\"아우디 q8\",],\n",
    "    \"KG 모빌리티\" : [\"렉스턴\",\"코란도\",\"토레스\",]\n",
    "}\n",
    "\n",
    "# 결과 출력\n",
    "prob = torch.softmax(output, dim=1).detach().cpu().squeeze().numpy()\n",
    "max_prob = max(prob)\n",
    "rounded_prob = round(max_prob, 2)\n",
    "\n",
    "class_names = [\"ev6\",\"ev9\",\"qm3\",\"qm6\",\"sm3\",\"sm6\",\"test\",\"xm3\",\"그랜저\",\"넥쏘\",\"더 뉴 아반떼\",\"렉스턴\",\"스타리아\",\"싼타페\",\"쏘나타\",\"아우디 a4\",\"아우디 a5\",\"아우디 a6\",\"아우디 a7\",\"아우디 e-tron\",\"아우디 q3\",\"아우디 q7\",\"아우디 q8\",\"아이오닉5\",\"아이오닉6\",\"제네시스 g70\",\"제네시스 g90\",\"제네시스 gv70\",\"코나\",\"코란도\",\"토레스\",\"투싼\"]  # 클래스 이름 리스트 (실제 클래스 이름으로 변경해야 함)\n",
    "predicted_class = class_names[predicted.item()] \n",
    "\n",
    "\n",
    "for brand, models in brandList.items():\n",
    "    if predicted_class in models:\n",
    "        print(f\"예측 브랜드: {brand}\")\n",
    "        break\n",
    "\n",
    "print(f\"예측 클래스: {predicted_class}\")\n",
    "print(f\"예측 확률: {rounded_prob*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchmetrics.functional as metrics\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from torchinfo import summary\n",
    "from torchvision.models import resnet18\n",
    "# 군용 항공기 종류 식별자\n",
    "AIRCRAFT_TYPE = {\n",
    "    'A':'Attater',\n",
    "    'B':'Bomber',\n",
    "    'C':'Cargo',\n",
    "    'F':'Fighter',\n",
    "    'AH':'Attack Helicopter',\n",
    "    'CH':'Cargo Helicopter',\n",
    "    'UH':'Utility Helicopter'\n",
    "}\n",
    "# 군용 항공기 제조국 식별자\n",
    "AIRCRAFT_NATION = [\n",
    "    'USA',\n",
    "    'EUROPE',\n",
    "    'RUSSIA',\n",
    "    'CHINA'\n",
    "]\n",
    "# 군용 항공기 명명 식별자\n",
    "AIRCRAFT_NAME = [\n",
    "    # USA\n",
    "    'A-10',\n",
    "    'B-1',\n",
    "    'B-2',\n",
    "    'B-52',\n",
    "    'C-5',\n",
    "    'C-17',\n",
    "    'C-130',\n",
    "    'CV-22',\n",
    "    'F-15',\n",
    "    'F-16',\n",
    "    'FA-18',\n",
    "    'F-22',\n",
    "    'F-35',\n",
    "    'AH-1',\n",
    "    'AH-64',\n",
    "    'CH-46',\n",
    "    'CH-47',\n",
    "    'CH-53',\n",
    "    'UH-60',\n",
    "    # Europe\n",
    "    'EF2000',\n",
    "    'Rafale',\n",
    "    'JAS-39',\n",
    "    'AV-8',\n",
    "    # Russia\n",
    "    'MIG-29',\n",
    "    'MIG-31',\n",
    "    'MIG-35',\n",
    "    'Su-30',\n",
    "    'Su-34',\n",
    "    'Su-57',\n",
    "    'Tu-160',\n",
    "    'Mi-8',\n",
    "    'Mi-24',\n",
    "    'Mi-28',\n",
    "    'Ka-52',\n",
    "    # CHINA\n",
    "    'J-15',\n",
    "    'J-20',\n",
    "    'H-6',\n",
    "    'Y-20',\n",
    "    'WZ-10',\n",
    "    # Unidentified\n",
    "    'UNIDENTIFIED'\n",
    "]\n",
    "len(AIRCRAFT_NAME)\n",
    "torch.random.manual_seed(40)\n",
    "data_path = './data/2/'\n",
    "\n",
    "for name in AIRCRAFT_NAME:\n",
    "    os.makedirs(os.path.join(data_path, name), exist_ok=True)\n",
    "transformer = transforms.Compose(transforms=[\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=[96, 96])]\n",
    ")\n",
    "root_path = './data/'\n",
    "\n",
    "ImgDS_1 = ImageFolder(root_path+'1/', transform=transformer)\n",
    "ImgDS_2 = ImageFolder(root_path+'2/', transform=transformer)\n",
    "for f, t in ImgDS_1:\n",
    "    print(f.shape, t)\n",
    "    break\n",
    "set(ImgDS_1.targets)\n",
    "set(ImgDS_2.targets)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 128, 3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, 3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*12*12, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "def makeLoader(self, batch=1, sampler=None):\n",
    "    train, valid, test = random_split(self, [0.7, 0.1, 0.2],\n",
    "                                      torch.Generator().manual_seed(40))\n",
    "    trainDL = DataLoader(train, batch_size=batch, sampler=sampler, shuffle=True)\n",
    "    validDL = DataLoader(valid, batch_size=batch, sampler=sampler, shuffle=True)\n",
    "    testDL = DataLoader(test, batch_size=batch, sampler=sampler, shuffle=True)\n",
    "    \n",
    "    return trainDL,validDL,testDL\n",
    "train_cost_list = []\n",
    "train_acc_list = []\n",
    "valid_cost_list = []\n",
    "valid_acc_list = []\n",
    "def train_model(model, opt, epochs, trainDL, validDL, schd=None):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for f, t in trainDL:\n",
    "            model.train()\n",
    "            sum_acc = 0\n",
    "            \n",
    "            h = model(f)\n",
    "            print(f.shape, t.shape, h.shape)\n",
    "            \n",
    "            cost = F.cross_entropy(h, t)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            cost.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            sum_acc += metrics.accuracy(h, t, task='multiclass', num_classes=model.layers[-1].out_features)\n",
    "            # print('.', end='')\n",
    "            \n",
    "        train_cost_list.append(cost)\n",
    "        train_acc_list.append(sum_acc / len(trainDL))\n",
    "        \n",
    "        for f, t in validDL:\n",
    "            model.eval()\n",
    "            sum_acc = 0\n",
    "            \n",
    "            h = model(f)\n",
    "            \n",
    "            cost = F.cross_entropy(h, t)\n",
    "            \n",
    "            sum_acc += metrics.accuracy(h, t, task='multiclass', num_classes=model.layers[-1].out_features)\n",
    "            \n",
    "        valid_cost_list.append(cost)\n",
    "        valid_acc_list.append(sum_acc / len(validDL))\n",
    "        \n",
    "        print(f'Epoch [{epoch:4}/{epochs:4}] ----------')\n",
    "        print(f'Train cost : {train_cost_list[-1]}, Train acc : {train_acc_list[-1]}')\n",
    "        print(f'Valid cost : {valid_cost_list[-1]}, Valid acc: {valid_acc_list[-1]}')\n",
    "        \n",
    "        if schd is not None:\n",
    "            schd.step(valid_cost_list[-1])\n",
    "        \n",
    "def test_model(model, testDL):\n",
    "    test_acc_list = []\n",
    "    test_f1_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    for f, t in testDL:\n",
    "        h = model(f)\n",
    "        \n",
    "        test_acc_list.append(metrics.accuracy(h, t, task='multiclass', num_classes=model.layers[-1].out_features))\n",
    "        test_f1_list.append(metrics.f1_score(h, t, task='multiclass', num_classes=model.layers[-1].out_features))\n",
    "        \n",
    "    print(f'Average Test acc : {sum(test_acc_list) / len(testDL)}')\n",
    "    print(f'Average Test f1  : {sum(test_f1_list) / len(testDL)}')\n",
    "def exportModel(model, filename):\n",
    "    pass\n",
    "def model_predict(model, input):\n",
    "    pass\n",
    "trainDL1, validDL1, testDL1 = makeLoader(ImgDS_1, 64)\n",
    "trainDL2, validDL2, testDL2 = makeLoader(ImgDS_2, 64)\n",
    "model1 = CNN(len(os.listdir('./data/1/')))\n",
    "model1.layers[-1].out_features\n",
    "model2 = CNN(len(AIRCRAFT_NAME))\n",
    "summary((model1))\n",
    "opt1 = optim.Adam(model1.parameters())\n",
    "opt2 = optim.Adam(model2.parameters())\n",
    "schd1 = optim.lr_scheduler.ReduceLROnPlateau(opt1)\n",
    "schd2 = optim.lr_scheduler.ReduceLROnPlateau(opt2)\n",
    "# train_model(model2, opt2, 50, trainDL2, validDL2, schd2)\n",
    "# torch.save(model2.state_dict(), 'H_best_model2.pth')\n",
    "\n",
    "# summary(model1)\n",
    "# resNet_1 = resnet18()\n",
    "# resNet_1.lay\n",
    "plt.plot([c.item() for c in train_cost_list])\n",
    "plt.plot([c.item() for c in valid_cost_list])\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.title('Model2 Cost')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
